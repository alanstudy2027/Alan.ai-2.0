
# ğŸ“„ Qwen 3 4B (MLX) + FAISS + PDF RAG Chatbot

A Retrieval-Augmented Generation (RAG) application that allows you to **chat with the contents of a PDF** using the **Qwen 3 4B** model in the **MLX architecture** for Apple Silicon.
The project uses **FAISS** as the vector database for fast semantic search.

---

## ğŸš€ Features

* **Local LLM** â€” Uses Qwen 3 4B in MLX for optimized performance on Apple Silicon.
* **RAG Pipeline** â€” Retrieval-Augmented Generation for more accurate and context-aware responses.
* **FAISS Vector Store** â€” Efficient semantic search for PDF content.
* **PDF Support** â€” Extracts text from PDF files using PyMuPDF (`fitz`).
* **No Internet Required** â€” Fully offline inference.

---

## ğŸ› ï¸ Tech Stack

* **LLM:** [Qwen 3 4B (MLX)](https://huggingface.co/Qwen)
* **Vector Database:** [FAISS](https://faiss.ai/)
* **PDF Processing:** [PyMuPDF](https://pymupdf.readthedocs.io/)
* **Embeddings:** [Sentence Transformers](https://www.sbert.net/)
* **Language:** Python

---

## ğŸ“‚ Project Structure

```
ğŸ“¦ qwen-pdf-rag
â”œâ”€â”€ main.py           # Main script for RAG-based chatbot
â”œâ”€â”€ requirements.txt  # Python dependencies
â”œâ”€â”€ README.md         # Project documentation
â””â”€â”€ sample.pdf        # Example PDF file for testing
```

---

## âš¡ Installation

1ï¸âƒ£ **Clone the repository**

```bash
git clone https://github.com/your-username/qwen-pdf-rag.git
cd qwen-pdf-rag
```

2ï¸âƒ£ **Create a virtual environment**

```bash
python -m venv .venv
source .venv/bin/activate
```

3ï¸âƒ£ **Install dependencies**

```bash
pip install -r requirements.txt
```

---

## â–¶ï¸ Usage

1ï¸âƒ£ **Place your PDF file** in the project folder.
2ï¸âƒ£ **Run the chatbot**

```bash
python main.py
```

3ï¸âƒ£ **Ask questions** about your PDF content in natural language.

---

## ğŸ§  How It Works

1. **Extract Text** â€” The PDF is read using `fitz` (PyMuPDF).
2. **Chunking** â€” The text is split into smaller, manageable chunks.
3. **Embedding** â€” Sentence Transformers encode chunks into embeddings.
4. **Indexing** â€” FAISS stores and indexes embeddings for semantic search.
5. **Querying** â€” Your question is embedded and searched against FAISS.
6. **LLM Response** â€” Qwen 3 4B (MLX) uses retrieved chunks to generate a contextual answer.

---

## ğŸ“¸ Demo

*(Add a screenshot or GIF of your chatbot running here)*

---

## ğŸ’¡ Future Improvements

* Web UI with **Streamlit** or **Gradio**
* Support for multiple PDFs
* Advanced chunking with metadata
* Caching for faster retrieval

---

## ğŸ¤ Contributing

Pull requests are welcome! For major changes, please open an issue to discuss your ideas.

---

## ğŸ“œ License

This project is licensed under the MIT License.

---

If you want, I can also make you a **LinkedIn post version** of this README so you can directly share it with a catchy tone. That way, itâ€™s GitHub + LinkedIn ready.
Do you want me to prepare that next?
